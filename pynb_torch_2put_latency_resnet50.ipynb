{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "CkQN8bVoXe1F",
        "outputId": "2f08fe99-b821-4620-e2fb-a82627bc4747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==2.2.0\n",
            "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.2.0) (3.16.1)\n",
            "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Installing collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "Successfully installed triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "3f727237d4ee44518ae0c8e9b0e81015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting validators\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m781.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators\n",
            "Successfully installed validators-0.34.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "!pip install triton==2.2.0\n",
        "!pip install validators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Using {device} for inference')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nukmyTjzXnnt",
        "outputId": "0e28893a-0cc5-49ba-89ac-652974961945"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu for inference\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model to the gpu\n",
        "load_start_time = time.time()\n",
        "resnet50.eval().to(device)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.synchronize() # Make sure all operations are finished\n",
        "load_end_time = time.time()"
      ],
      "metadata": {
        "id": "mPpBMFebXs3F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load samples to the gpu\n",
        "uris = [\n",
        "    'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',\n",
        "    'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',\n",
        "    'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',\n",
        "    'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',\n",
        "]\n",
        "batch = torch.cat(\n",
        "    [utils.prepare_input_from_uri(uri) for uri in uris]\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "QV6KgksIqVY-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start inferencing\n",
        "total_samples = 0\n",
        "inference_start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    output = torch.nn.functional.softmax(resnet50(batch), dim=1)\n",
        "    total_samples += batch.size(0)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.synchronize() # Make sure all operations are finished\n",
        "inference_stop_time = time.time()"
      ],
      "metadata": {
        "id": "qwPZzue6qhXQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the result\n",
        "inference_time = inference_stop_time - inference_start_time\n",
        "throughput = total_samples / inference_time\n",
        "\n",
        "print(f\"Total samples : {total_samples}\")\n",
        "print(f\"Throughput: {throughput:.2f} samples/second\")\n",
        "print(f\"Load model latency: {load_end_time - load_start_time}\")\n",
        "print(f\"Inference latency: {inference_time}\")\n",
        "\n",
        "results = utils.pick_n_best(predictions=output, n=5)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Pde7K9qj-B",
        "outputId": "11af5e32-e54b-4db8-92f2-20180e204351"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples : 4\n",
            "Throughput: 4.67 samples/second\n",
            "Load model latency: 0.03139019012451172\n",
            "Inference latency: 0.8567917346954346\n",
            "Downloading Imagenet Classes names.\n",
            "Downloading finished.\n",
            "sample 0: [('laptop, laptop computer', '34.7%'), ('mouse, computer mouse', '11.9%'), ('notebook, notebook computer', '10.9%'), ('monitor', '3.9%'), ('web site, website, internet site, site', '3.1%')]\n",
            "sample 1: [('mashed potato', '78.6%'), ('broccoli', '7.3%'), ('meat loaf, meatloaf', '2.5%'), ('plate', '1.8%'), ('guacamole', '0.2%')]\n",
            "sample 2: [('racket, racquet', '25.4%'), ('tennis ball', '6.0%'), ('ping-pong ball', '2.0%'), ('catamaran', '0.3%'), ('bathtub, bathing tub, bath, tub', '0.3%')]\n",
            "sample 3: [('groenendael', '12.7%'), ('Scottish deerhound, deerhound', '11.2%'), ('flat-coated retriever', '6.6%'), ('kelpie', '6.1%'), ('Great Dane', '3.1%')]\n",
            "[[('laptop, laptop computer', '34.7%'), ('mouse, computer mouse', '11.9%'), ('notebook, notebook computer', '10.9%'), ('monitor', '3.9%'), ('web site, website, internet site, site', '3.1%')], [('mashed potato', '78.6%'), ('broccoli', '7.3%'), ('meat loaf, meatloaf', '2.5%'), ('plate', '1.8%'), ('guacamole', '0.2%')], [('racket, racquet', '25.4%'), ('tennis ball', '6.0%'), ('ping-pong ball', '2.0%'), ('catamaran', '0.3%'), ('bathtub, bathing tub, bath, tub', '0.3%')], [('groenendael', '12.7%'), ('Scottish deerhound, deerhound', '11.2%'), ('flat-coated retriever', '6.6%'), ('kelpie', '6.1%'), ('Great Dane', '3.1%')]]\n"
          ]
        }
      ]
    }
  ]
}